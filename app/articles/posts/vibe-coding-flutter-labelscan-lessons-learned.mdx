---
title: Vibe Coding My Way Through Flutter: Building LabelScan and Lessons Learned
publishedAt: 2025-04-03
summary: A practical journey building Label Scan, a Flutter app using AI for real-time price scanning, and the key lessons learned about Flutter development along the way.
category: Mobile Development
tags: Flutter, Dart, Mobile Development, AI, OCR, Case Study, Learning, Riverpod, Flask
---

## Introduction

Ever find yourself at the grocery store checkout, bracing for the final tally? Keeping track of your spending *while* you shop can be a hassle. That's the exact problem I set out to solve with LabelScan, a mobile app designed to scan price tags and maintain a running total of your cart's cost in real-time.

This wasn't a meticulously planned project from the start; it was more of a "vibe coding" adventure. I had an idea, saw Flutter as an interesting tool, and dove in. Along the way, I built a functional app leveraging AI for price extraction and learned a *ton* about Flutter development – the good, the challenging, and the surprising. This article shares that journey, focusing on the practical aspects of building LabelScan and the key Flutter takeaways I gathered.

## Why LabelScan? The Problem & The Flutter Solution

The core motivation was simple: I wanted a way to avoid checkout shock. Manually adding items on a calculator felt clunky, and existing budgeting apps didn't quite fit the real-time, in-store scanning need I envisioned. I needed something fast, visual, and mobile-first.

So, why Flutter? Honestly, it was an experiment. I'd never worked with Flutter or Dart before, and I wanted to see if I could essentially "vibe code" my way through building a functional app in an unfamiliar ecosystem. It was a challenge to myself: could I learn and build simultaneously?

## Building the Frontend: Key Flutter Aspects

Flutter's promise is beautiful, natively compiled applications for mobile, web, and desktop from a single codebase. For LabelScan, the focus was mobile. Here's how some key frontend pieces came together:

### UI & State Management

Flutter's declarative UI paradigm, built around widgets, was central. The main screens involved displaying the camera feed, a list of scanned items with their prices, and the running subtotal, tax, and final cost. Initially, I relied on Flutter's built-in `setState` to manage the app's state – the list of scanned items, the running totals, etc. However, as the app's complexity grew, I found `setState` becoming a bit unwieldy for managing state across different parts of the widget tree.

This led me to refactor and implement `Riverpod`, a popular state management library in the Flutter ecosystem. Riverpod provided a more structured and scalable way to handle dependencies and state, which was a significant learning point in itself.

### Camera Integration

The core feature! The app needed to access the phone's camera to capture images of price labels. I started with the `camera` package, which provides fine-grained control over the device camera. Getting the camera preview working and handling permissions was relatively straightforward thanks to the package's documentation.

Later, as I got more comfortable and started adding features ("feature creep" in the best way!), I also integrated the `image_picker` package. This allowed users to upload profile avatars and even select existing photos from their gallery to scan for prices, adding flexibility beyond the live camera feed.

### Networking

The Flutter app didn't do the heavy lifting of OCR itself. It needed to send the captured image to the Python/Flask backend and receive the extracted price data. For this, I used Flutter's standard `http` package. Since I was connecting to a Flask microservice that I had essentially repurposed for this project, the networking layer was quite direct. Sending the image data as part of the request and parsing the JSON response containing the price was handled effectively by the `http` package without needing more complex libraries like `dio` for this particular use case.

## Connecting to the Brains: Backend & AI Interaction

While Flutter handled the user-facing experience, the intelligence resided in the backend. A Python Flask server acted as the intermediary:

1.  The Flutter app sent the captured image data (from the `camera` or `image_picker`) to a specific API endpoint (e.g., `/api/extract_data`).
2.  The Flask backend received this image.
3.  The backend then invoked Google Cloud Vision and potentially the Gemini API (as mentioned in the README) to perform Optical Character Recognition (OCR) on the image, specifically looking for price-like patterns.
4.  Once the price was (hopefully) extracted, the backend sent it back to the Flutter app in a JSON response.
5.  The Flutter app received the response, parsed the price using the `http` package, and updated the UI using Riverpod state management to add the item and recalculate the totals.

This client-server separation kept the Flutter app relatively lightweight, focusing on UI and user interaction, while the computationally intensive OCR task was handled server-side.

## Lessons from the "Vibe Code": Flutter Development Takeaways

Diving into Flutter without prior experience was certainly an adventure. "Vibe coding" my way through LabelScan taught me several things:

*   **The Biggest Hurdle:** While much of the process felt smooth, the trickiest part technically was handling the image upload correctly within the Flutter app itself. Ensuring the image data was properly formatted, setting the correct MIME types for the HTTP request to the backend, and managing the asynchronous nature of picking/capturing and uploading took some careful handling and debugging. Outside of that specific challenge, the end-to-end build felt surprisingly quick, though wrestling with slow or clunky Android simulators was sometimes a drag on the iteration speed.

*   **Pleasant Surprises - The "Native" Feel & DX:** What stood out positively was how "native" the resulting app felt. Flutter's declarative UI, built by composing widgets, translated into an interface that felt intuitive and performed smoothly on the device. The developer experience (DX) was also a highlight. Features like hot reload (though sometimes needing a nudge, see below) sped up UI tweaks, and I found the Flutter logs to be exceptionally clear and helpful for debugging – a solid 10/10 experience there.

*   **"Aha!" Moment / Pro Tip:** A simple but crucial lesson learned, especially when iterating quickly: Make absolutely sure the app running in your simulator or on your device is the *current* build from your latest `flutter run` command! More than once, I found myself debugging an issue that wasn't actually in my latest code, but persisted from a previous session running in memory. Killing the old process and ensuring a fresh run saved some head-scratching.

*   **Flutter's Suitability:** Based on this project, Flutter feels highly suitable for rapidly building apps that require native device features like camera access and network communication. Its cross-platform nature is a huge plus. While I can imagine potential performance bottlenecks in highly complex scenarios (as with any abstraction layer), the development speed and the quality of the end result were impressive. For cross-platform development, especially for projects like LabelScan, I'd definitely recommend Flutter. My hunch is that the performance overhead might even be more favorable compared to alternatives like React Native, though I didn't benchmark directly.

## Conclusion

Building LabelScan was a practical dive into the world of Flutter, driven by curiosity and the desire to solve a simple, everyday problem. From integrating camera functionality and managing state with Riverpod to connecting with a Python backend for AI-powered OCR, the project touched upon many facets of modern mobile app development.

The "vibe coding" approach, while perhaps unconventional, proved to be an effective way to learn Flutter hands-on. The framework's excellent developer experience, rich widget library, and strong community support made tackling an unfamiliar ecosystem achievable and even enjoyable. While challenges like image handling and simulator quirks existed, the overall journey was positive, resulting in a useful tool and valuable insights into Flutter's capabilities.

If you're curious to see the code behind LabelScan, you can check out the repository on GitHub: [https://github.com/anthonycoffey/flutter-labelscan](https://github.com/anthonycoffey/flutter-labelscan)
